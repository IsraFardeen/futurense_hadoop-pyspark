#PySpark wordcount example
rdd1 = sc.textFile("/home/ubuntu/futurense_hadoop-pyspark/labs/dataset/wordcount/wordcount-input.txt")
rdd2 = lines.flatMap(lambda x: x.split(' '))
rdd3 = rdd2.map(lambda x: (x, 1))
rdd4 = rdd3.reduceByKey(lambda a, b: a + b)
output = counts.collect()
for (word, count) in output:
	print("%s: %i" % (word, count))
